import streamlit as st
from PIL import Image
import google.generativeai as genai

# --- é…ç½® (å»ºè®®åœ¨ Streamlit Secrets ä¸­é…ç½®) ---
# ä¿®æ”¹ä»£ç ç¬¬ 6-11 è¡Œå·¦å³
if "GEMINI_API_KEY" in st.secrets:
    API_KEY = st.secrets["GEMINI_API_KEY"]
else:
    # ç¡®ä¿è¿™é‡Œçš„ Key æ˜¯ä½ ä» Google AI Studio å¤åˆ¶çš„æœ€æ–° Key
    API_KEY = "AIzaSyCqOqb1OLQcO3XdFP0JRz_HlBt13gGfhvo" 

genai.configure(api_key=API_KEY)
# ç¡®ä¿ä½¿ç”¨æœ€é€šç”¨çš„æ¨¡å‹åç§°
try:
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æƒé™æˆ–æ¨¡å‹åç§°: {e}")

st.set_page_config(page_title="AI éšèº«ç¿»è¯‘å®˜", page_icon="ğŸ¤")

# --- æ ¸å¿ƒ JavaScript é€»è¾‘ï¼šè¯­éŸ³è¯†åˆ« + è¯­éŸ³åˆæˆ ---
def st_speech_interaction():
    js_code = """
    <script>
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'zh-CN'; // ç›‘å¬ä¸­æ–‡
    
    // æœ—è¯»å‡½æ•°
    function speak(text) {
        const msg = new SpeechSynthesisUtterance(text);
        msg.lang = 'en-US';
        window.speechSynthesis.speak(msg);
    }

    // å½“ç”¨æˆ·ç‚¹å‡»æŒ‰é’®æ—¶å¯åŠ¨è¯†åˆ«
    window.parent.document.addEventListener('start_speech', () => {
        recognition.start();
    });

    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        // æŠŠè¯†åˆ«åˆ°çš„ä¸­æ–‡ä¼ å› Streamlit
        const streamlit_input = window.parent.document.querySelector('textarea');
        if (streamlit_input) {
            streamlit_input.value = text;
            streamlit_input.dispatchEvent(new Event('input', {bubbles: true}));
        }
    };
    </script>
    """
    st.components.v1.html(js_code, height=0)

# --- UI ç•Œé¢ ---
tab1, tab2 = st.tabs(["ğŸ“¸ æ‹ç…§è¯†ç‰©", "ğŸ¤ è¯­éŸ³ç¿»è¯‘"])

# --- Tab 1: æ‹ç…§è¯†ç‰© (ä¿ç•™åŸæœ‰åŠŸèƒ½) ---
with tab1:
    img_file = st.camera_input("æ‹ç…§è¯†åˆ«ç‰©ä½“")
    if img_file:
        img = Image.open(img_file)
        with st.spinner('åˆ†æä¸­...'):
            prompt = "Identify this object. Provide: 1. Word 2. Chinese Translation 3. Example sentence."
            res = model.generate_content([prompt, img])
            st.write(res.text)

# --- Tab 2: è¯­éŸ³å¯¹è¯ ---
with tab2:
    st.write("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¹¶è¯´ä¸­æ–‡ï¼Œæˆ‘ä¼šç”¨è‹±æ–‡å›ç­”ä½ å¹¶æœ—è¯»ã€‚")
    
    # è¾“å…¥æ¡†ï¼Œç”¨äºæ¥æ”¶è¯­éŸ³è½¬æ–‡å­—çš„ç»“æœ
    user_query = st.text_area("è¯†åˆ«åˆ°çš„ä¸­æ–‡ï¼š", key="voice_input", placeholder="æ­£åœ¨å¬ä½ è¯´...")

    if st.button("ğŸ”´ æŒ‰ä¸‹å¼€å§‹è¯´è¯"):
        # è§¦å‘ JS å¼€å§‹å½•éŸ³
        st.components.v1.html("<script>window.parent.dispatchEvent(new Event('start_speech'));</script>", height=0)

    if user_query:
        with st.spinner('æ€è€ƒä¸­...'):
            # è¿™é‡Œçš„ Prompt å¼ºåˆ¶ AI ç”¨è‹±æ–‡å›ç­”
            prompt = f"The user said in Chinese: '{user_query}'. Please translate/respond to this in natural English ONLY. Keep it conversational."
            response = model.generate_content(prompt)
            answer = response.text
            
            st.subheader("AI è‹±æ–‡å›å¤ï¼š")
            st.info(answer)
            
            # è‡ªåŠ¨æœ—è¯»å›å¤å†…å®¹
            st.components.v1.html(f"""
                <script>
                var msg = new SpeechSynthesisUtterance(`{answer.replace('`','')}`);
                msg.lang = 'en-US';
                window.speechSynthesis.speak(msg);
                </script>
            """, height=0)

st_speech_interaction()
